{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_ass_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLBMiGhgQGdAfjDLxyBz7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishmcsa/Assigment-7/blob/main/model_ass_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpJLwuGYv9uS"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi3hCU0pxSVU",
        "outputId": "1c356ebf-8fe2-4455-aaad-3416df80613b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx0q6QhtxmZc",
        "outputId": "22dee505-25ab-4073-ac61-c7125a026693"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Assigment6\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Assigment6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HEgtF-Swn01"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "    def __init__(self, args):\r\n",
        "        super(Net, self).__init__()\r\n",
        "\r\n",
        "        self.layer1 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(32),\r\n",
        "            nn.Dropout(args.dropout_value),  # In: 32x32x3 | Out: 32x32x32 | RF: 3x3\r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(32), # In: 32x32x32 | Out: 32x32x32 | RF: 5x5\r\n",
        "        )\r\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # In: 32x32x32 | Out: 16x16x32 | RF: 6x6\r\n",
        "        self.layer2 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(64),\r\n",
        "            nn.Dropout(args.dropout_value),  # In: 16x16x32 | Out: 16x16x64 | RF: 10x10\r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(64), # In: 16x16x64 | Out: 16x16x64 | RF: 14x14\r\n",
        "        )\r\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # In: 16x16x64 | Out: 8x8x64 | RF:16x16\r\n",
        "        self.layer3 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1, groups=64, bias=False),\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1, 1), padding=0, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(64),\r\n",
        "            nn.Dropout(args.dropout_value),  # In: 8x8x64 | Out: 8x8x64 | RF: 24x24\r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(64), # In: 8x8x64 | Out: 8x8x64 | RF: 32x32\r\n",
        "        )\r\n",
        "        self.pool3 = nn.MaxPool2d(2, 2) # In: 8x8x64 | Out: 4x4x64 | RF: 36x36\r\n",
        "        self.layer4 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1, dilation=2, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(128),\r\n",
        "            nn.Dropout(args.dropout_value),  # In: 4x4x64 | Out: 4x4x128 | RF: 68x68\r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(128),  # In: 4x4x128 | Out: 4x4x128 | RF: 84x84\r\n",
        "        )\r\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)  # In: 4x4x128 | Out: 1x1x128 | RF: 108x108\r\n",
        "        self.layer5 = nn.Sequential(\r\n",
        "            nn.Linear(in_features=128, out_features=10),\r\n",
        "            # nn.ReLU() NEVER!\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.pool1(x)\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.pool2(x)\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.pool3(x)\r\n",
        "        x = self.layer4(x)\r\n",
        "        x = self.gap(x)\r\n",
        "        x = x.view(-1, 128)\r\n",
        "        x = self.layer5(x)\r\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    }
  ]
}